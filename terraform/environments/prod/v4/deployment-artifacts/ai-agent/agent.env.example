# =============================================================================
# Production v4 AI Agent Environment Variables
# =============================================================================

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 환경별 변경 필요
LIVEKIT_URL=wss://livekit.sodam.store
API_BASE_URL=http://localhost:8080
REDIS_URL=redis://<RDS_ELASTICACHE_ENDPOINT>:6379
AGENT_NAME=voice-agent
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 환경별 변경 필요

# LiveKit connection (used by the LiveKit agent SDK)
LIVEKIT_API_KEY=<YOUR_LIVEKIT_API_KEY>
LIVEKIT_API_SECRET=<YOUR_LIVEKIT_API_SECRET>

# AWS Credentials (for Polly TTS)
# EC2 IAM Role을 사용하므로 필요 없을 수 있음
AWS_ACCESS_KEY_ID=<YOUR_AWS_ACCESS_KEY>
AWS_SECRET_ACCESS_KEY=<YOUR_AWS_SECRET_KEY>
AWS_DEFAULT_REGION=ap-northeast-2

# ===============================================
# STT Provider Configuration (v4: AI Server)
# ===============================================
# Provider: external (AI Server) | aws (Transcribe)
STT_PROVIDER=external
AI_SERVER_URL=http://localhost:8001

# ===============================================
# TTS Provider Configuration
# ===============================================
# Provider: aws (Polly) | external (AI Server)
TTS_PROVIDER=aws
TTS_VOICE=Seoyeon

# Agent authentication for API calls
API_INTERNAL_TOKEN=<YOUR_API_INTERNAL_TOKEN>

# RAG (Conversation Search) - Optional
# If you want to use conversation history search in your agent
# The RAG service uses the same API_BASE_URL and API_INTERNAL_TOKEN configured above

# RAG Parent-Child Configuration (Optional - uses defaults if not set)
RAG_MEMORY_SIMILARITY_THRESHOLD=0.35    # Similarity threshold for memory recall (0.0-1.0, lower = more results)
RAG_CHILD_CHUNK_SIZE=200                # Child chunk size (must match ops-api setting)
RAG_CHILD_CHUNK_OVERLAP=50              # Child chunk overlap (must match ops-api setting)
RAG_WINDOW_CONTEXT_CHARS=150            # Window size for snippet extraction (must match ops-api setting)

# Agent Timezone
AGENT_TIMEZONE=Asia/Seoul               # Timezone for relative time display

# ===============================================
# KMA (Korea Meteorological Administration) API
# ===============================================
# Get your API key from https://apihub.kma.go.kr/
KMA_AUTH_KEY=<YOUR_KMA_AUTH_KEY>
KMA_MCP_URL=http://localhost:8002/sse   # MCP server URL (SSE endpoint)
MCP_ENABLED=true

# Optional
LOG_LEVEL=INFO

# ===============================================
# LLM Provider Configuration (v4: vLLM + Qwen3)
# ===============================================
# Provider: openai (vLLM) | aws (Bedrock) | ollama
LLM_PROVIDER=openai

# Model name (provider-specific)
# - vLLM: Qwen/Qwen3-8B-AWQ
# - AWS: global.anthropic.claude-haiku-4-5-20251001-v1:0
# - Ollama: exaone3.5:7.8b (Korean optimized with tool calling)
LLM_MODEL=Qwen/Qwen3-8B-AWQ

# Base URL (for vLLM/ollama providers)
# - vLLM (Docker with host network): http://localhost:8000/v1
# - Ollama: http://localhost:11434/v1
LLM_BASE_URL=http://localhost:8000/v1

# Temperature (default: 0.7)
LLM_TEMPERATURE=0.7

# ===============================================
# Token Management (v4)
# ===============================================
MAX_CONTEXT_ITEMS=20  # 컨텍스트 아이템 최대 개수 (토큰 오버플로우 방지)
