# =============================================================================
# Production v3 AI Agent Environment Variables
# =============================================================================

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 환경별 변경 필요
LIVEKIT_URL=wss://livekit.sodam.store
API_BASE_URL=http://localhost:8080
REDIS_URL=redis://<RDS_ELASTICACHE_ENDPOINT>:6379
AGENT_NAME=voice-agent
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 환경별 변경 필요

# LiveKit connection (used by the LiveKit agent SDK)
LIVEKIT_API_KEY=<YOUR_LIVEKIT_API_KEY>
LIVEKIT_API_SECRET=<YOUR_LIVEKIT_API_SECRET>

# AWS Credentials (for Polly TTS, Bedrock LLM)
# EC2 IAM Role을 사용하므로 필요 없을 수 있음
AWS_ACCESS_KEY_ID=<YOUR_AWS_ACCESS_KEY>
AWS_SECRET_ACCESS_KEY=<YOUR_AWS_SECRET_KEY>
AWS_DEFAULT_REGION=ap-northeast-2

# ===============================================
# STT Provider Configuration
# ===============================================
# Provider options: aws | custom
STT_PROVIDER=custom
# STT 서버 URL (custom 사용 시 필수, docker-compose의 stt 서비스)
STT_SERVER_URL=http://localhost:8000

# Agent authentication for API calls
API_INTERNAL_TOKEN=<YOUR_API_INTERNAL_TOKEN>

# RAG (Conversation Search) - Optional
# If you want to use conversation history search in your agent
# The RAG service uses the same API_BASE_URL and API_INTERNAL_TOKEN configured above

# RAG Parent-Child Configuration (Optional - uses defaults if not set)
RAG_MEMORY_SIMILARITY_THRESHOLD=0.35    # Similarity threshold for memory recall (0.0-1.0, lower = more results)
RAG_CHILD_CHUNK_SIZE=200                # Child chunk size (must match ops-api setting)
RAG_CHILD_CHUNK_OVERLAP=50              # Child chunk overlap (must match ops-api setting)
RAG_WINDOW_CONTEXT_CHARS=150            # Window size for snippet extraction (must match ops-api setting)

# Agent Timezone
AGENT_TIMEZONE=Asia/Seoul               # Timezone for relative time display

# KMA (Korea Meteorological Administration) API
# Get your API key from https://apihub.kma.go.kr/
KMA_AUTH_KEY=<YOUR_KMA_AUTH_KEY>
KMA_MCP_URL=http://localhost:8001/sse   # MCP server URL (SSE endpoint)
MCP_ENABLED=false # true | false

# Optional
LOG_LEVEL=INFO

# ===============================================
# LLM Provider Configuration (Pluggable)
# ===============================================
# Provider options: aws | ollama
LLM_PROVIDER=ollama

# Model name (provider-specific)
# - AWS: global.anthropic.claude-haiku-4-5-20251001-v1:0
# - Ollama: exaone3.5:7.8b (Korean optimized with tool calling)
LLM_MODEL=exaone3.5:7.8b

# Base URL (for ollama/openai providers)
# - Ollama (Docker with host network): http://localhost:11434/v1
LLM_BASE_URL=http://localhost:11434/v1

# Temperature (default: 0.7)
LLM_TEMPERATURE=0.7
